
SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models

SVDQUANT: ABSORBING OUTLIERS BY LOW-RANK
COMPONENTS FOR 4-BIT DIFFUSION MODELS

Muyang Li1∗‡ Yujun Lin1∗ Zhekai Zhang1† Tianle Cai4 Xiuyu Li5‡
Junxian Guo1,6 Enze Xie2 Chenlin Meng7 Jun-Yan Zhu3 Song Han1,2

1MIT 2NVIDIA 3CMU 4Princeton 5UC Berkeley 6SJTU 7Pika Labs
https://hanlab.mit.edu/projects/svdquant

Prompt: a cyberpunk cat holding a huge neon sign that says "SVDQuant is lite and fast", wearing fancy goggles and a black leather jacket.

FLUX.1-dev BF16 
(25 Steps) 

DiT Memory: 22.7 GiB 
E2E Latency: 111.7 s

NF4 (W4A16) 
LPIPS: 0.272 

DiT Memory: 6.9 GiB (3.3× Less) 
E2E Latency: 38.6 s (2.9× Faster)

Naïve INT4 (W4A4) 
LPIPS: 0.327 

DiT Memory: 6.3 GiB (3.6× Less) 
E2E Latency: 12.5 s (8.9× Faster)

SVDQuant INT4 (W4A4) 
LPIPS: 0.254 

DiT Memory: 6.5 GiB (3.5× Less) 
E2E Latency: 12.9 s (8.7× Faster)

PixArt-  FP16 
(20 Steps)

Σ ViDiT-Q (W4A8) 
LPIPS: 0.573

Naïve INT4 (W4A4) 
LPIPS: 0.708

SVDQuant INT4 (W4A4) 
LPIPS: 0.326

Prompt: medium rare steak tenderloin super tasty photo.

Figure 1: SVDQuant is a post-training quantization technique for 4-bit weights and activations that well maintains
visual fidelity. On 12B FLUX.1-dev, it achieves 3.6× memory reduction compared to the BF16 model. By
eliminating CPU offloading, it offers 8.7× speedup over the 16-bit model when on a 16GB laptop 4090 GPU, 3×
faster than the NF4 W4A16 baseline. On PixArt-Σ, it demonstrates significantly superior visual quality over other
W4A4 or even W4A8 baselines. “E2E” means the end-to-end latency including the text encoder and VAE decoder.

ABSTRACT

Diffusion models have been proven highly effective at generating high-quality
images. However, as these models grow larger, they require significantly
more memory and suffer from higher latency, posing substantial challenges for
deployment. In this work, we aim to accelerate diffusion models by quantizing
their weights and activations to 4 bits. At such an aggressive level, both weights
and activations are highly sensitive, where conventional post-training quantization
methods for large language models like smoothing become insufficient. To over-
come this limitation, we propose SVDQuant, a new 4-bit quantization paradigm.
Different from smoothing which redistributes outliers between weights and
activations, our approach absorbs these outliers using a low-rank branch. We first
consolidate the outliers by shifting them from activations to weights, then employ
a high-precision low-rank branch to take in the weight outliers with Singular Value
Decomposition (SVD). This process eases the quantization on both sides. However,
naïvely running the low-rank branch independently incurs significant overhead
due to extra data movement of activations, negating the quantization speedup. To
address this, we co-design an inference engine Nunchaku that fuses the kernels of

∗Algorithm co-lead. † System lead. ‡ Part of the work done during an internship at NVIDIA.

1

ar
X

iv
:2

41
1.

05
00

7v
1 

 [
cs

.C
V

] 
 7

 N
ov

 2
02

4

https://hanlab.mit.edu/projects/svdquant


SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models

the low-rank branch into those of the low-bit branch to cut off redundant memory
access. It can also seamlessly support off-the-shelf low-rank adapters (LoRAs)
without the need for re-quantization. Extensive experiments on SDXL, PixArt-Σ,
and FLUX.1 validate the effectiveness of SVDQuant in preserving image quality.
We reduce the memory usage for the 12B FLUX.1 models by 3.5×, achieving 3.0×
speedup over the 4-bit weight-only quantized baseline on the 16GB laptop 4090
GPU, paving the way for more interactive applications on PCs. Our quantization
library* and inference engine† are open-sourced.

1 INTRODUCTION

Diffusion models have shown remarkable capabilities in generating high-quality images (Ho et al.,
2020), with recent advances further enhancing user control over the generation process. Trained
on vast data, these models can create stunning images from simple text prompts, unlocking diverse
image editing and synthesis applications (Meng et al., 2022b; Ruiz et al., 2023; Zhang et al., 2023).

C
om

pu
ta

tio
n 

(T
M

A
C

s)

0

10

20

30

40

50

Parameters (B)
0 3 6 9 12 15

Diffusion Model
LLM AuraFlow v0.1’24

FLUX.1’24

Llama2-13B’23

Gemma2-9B’24
Llama3-8B’24

Phi-3.5’24
Gemma2-2B’24

PixArt’23

SD3-Medium’24

SDXL’23

SD1.4’22

Figure 2: Computation vs. param-
eters for LLMs and diffusion mod-
els. LLMs’ computation is mea-
sured with 512 context and 256
output tokens, and diffusion mod-
els’ computation is for a single
step. Dashed lines show trends.

To pursue higher image quality and more precise text-to-image
alignment, researchers are increasingly scaling up diffusion mod-
els. As shown in Figure 2, Stable Diffusion (SD) (Rombach et al.,
2022) 1.4 only has 800M parameters, while SDXL (Podell et al.,
2024) scales this up to 2.6B parameters. AuraFlow v0.1 (fal.ai,
2024) extends this further to 6B parameters, with the latest model,
FLUX.1 (Black-Forest-Labs, 2024), pushing the boundary to 12B
parameters. Compared to large language models (LLMs), diffusion
models are significantly more computationally intensive. Their com-
putational costs‡ increase more rapidly with model size, posing a
prohibitive memory and latency barrier for real-world model deploy-
ment, particularly for interactive use cases that demand low latency.

As Moore’s law slows down, hardware vendors are turning to low-
precision inference to sustain performance improvements. For in-
stance, NVIDIA’s Blackwell Tensor Cores introduce a new 4-bit
floating point (FP4) precision, doubling the performance compared
to FP8 (NVIDIA, 2024). Therefore, using 4-bit inference to acceler-
ate diffusion models is appealing. In the realm of LLMs, researchers have leveraged quantization to
compress model sizes and boost inference speed (Dettmers et al., 2022; Xiao et al., 2023). However,
unlike LLMs–where latency is primarily constrained by loading model weights, especially with small
batch sizes–diffusion models are heavily computationally bound, even with a single batch. As a result,
weight-only quantization cannot accelerate diffusion models. To achieve speedup, both weights and
activations must be quantized to the same bit width; otherwise, the lower-precision side will be upcast
during computation, negating potential performance enhancements.

In this work, we focus on quantizing both the weights and activations of diffusion models to 4
bits. This challenging and aggressive scheme is often prone to severe quality degradation. Existing
methods like smoothing (Xiao et al., 2023; Lin et al., 2024a), which attempt to transfer the outliers
between the weights and activations, are less effective since both sides are highly vulnerable to
outliers. To address this issue, we propose a new general-purpose quantization paradigm, SVDQuant.
Our core idea is to introduce a low-cost branch to absorb outliers on both sides. To achieve this, as
illustrated in Figure 3, we first aggregate the outliers by migrating them from activation X to weight
W via smoothing. Then we apply Singular Value Decomposition (SVD) to the updated weight,
Ŵ , decomposing it into a low-rank branch L1L2 and a residual Ŵ −L1L2. The low-rank branch
operates at 16 bits, allowing us to quantize only the residual to 4 bits, which has significantly reduced
outliers and magnitude. However, naively running the low-rank branch separately incurs substantial
memory access overhead, offsetting the speedup of 4-bit inference. To overcome this, we co-design a
specialized inference engine Nunchaku, which fuses the low-rank branch computation into the 4-bit

*Quantization library: github.com/mit-han-lab/deepcompressor
†Inference Engine: github.com/mit-han-lab/nunchaku
‡Computational cost is measured by number of Multiply-Accumulate operations (MACs). 1 MAC=2 FLOPs.

2

https://github.com/mit-han-lab/deepcompressor
https://github.com/mit-han-lab/deepcompressor
https://github.com/mit-han-lab/nunchaku
https://github.com/mit-han-lab/deepcompressor
https://github.com/mit-han-lab/nunchaku


SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models

Q
ua

nt
iz

at
io

n 
Le

ve
l

0

10

0

2
 |X |  |W |Outlier

Low Effective Bits

Very Hard to Quantize Hard to Quantize

0

2

0

0.45
 | X̂ |  |Ŵ |

Migrate Difficulty with Smoothing

Easy to Quantize Harder to Quantize

0

2

0

0.05

rank=32

16-Bit  L1

16-Bit  L2

+

 | X̂ |  |Ŵ − L1L2 |

Migrate Difficulty with SVD

Easy to Quantize Easy to Quantize No Need to Quantize

Low-Rank Branch  L1L2

(a) Original (b) Shift Outliers from Activation   to Weight  X W (c) SVDQuant (Ours)

Channel Input Channel Channel Input Channel Channel Input Channel

Figure 3: Overview of SVDQuant. (a) Originally, both the activation X and weight W contain outliers, making
4-bit quantization challenging. (b) We migrate the outliers from the activation to weight, resulting in the updated
activation X̂ and weight Ŵ . While X̂ becomes easier to quantize, Ŵ now becomes more difficult. (c)
SVDQuant further decomposes Ŵ into a low-rank component L1L2 and a residual Ŵ − L1L2 with SVD.
Thus, the quantization difficulty is alleviated by the low-rank branch, which runs at 16-bit precision.

quantization and computation kernels. This design enables us to achieve measured inference speedup
even with additional branches.

SVDQuant can quantize various text-to-image diffusion architectures, including both UNet (Ho
et al., 2020; Ronneberger et al., 2015) and DiT (Peebles & Xie, 2023) backbones, into 4 bits, while
maintaining visual quality. It supports both INT4 and FP4 data types, and integrates seamlessly
with pre-trained low-rank adapters (LoRA) (Hsu et al., 2022) without requiring re-quantization. To
our knowledge, we are the first to successfully apply 4-bit post-training quantization to both the
weights and activations of diffusion models, and achieve measured speedup on NVIDIA GPUs. On
the latest 12B FLUX.1, we largely preserve the image quality and reduce the memory footprint of
the original BF16 model by 3.5× and deliver a 3.0× speedup over the NF4 weight-only quantized
baseline, measured on a 16GB laptop-level RTX4090 GPU. See Figure 1 for visual examples.

2 RELATED WORK

Diffusion models. Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020) have emerged as
a powerful class of generative models, known for their ability to generate high-quality samples by
modeling the data distribution through an iterative denoising process. Recent advancements in text-
to-image diffusion models (Balaji et al., 2022; Rombach et al., 2022; Podell et al., 2024) have already
revolutionized content generation. Researchers further shifted from convolution-based UNet architec-
tures (Ronneberger et al., 2015; Ho et al., 2020) to transformers (e.g., DiT (Peebles & Xie, 2023) and
U-ViT (Bao et al., 2023)) and scaled up the model size (Esser et al., 2024). However, diffusion models
suffer from extremely slow inference speed due to their long denoising sequences and intense compu-
tation. To address this, various approaches have been proposed, including few-step samplers (Zhang
& Chen, 2022; Zhang et al., 2022; Lu et al., 2022) or distilling fewer-step models from pre-trained
ones (Salimans & Ho, 2021; Meng et al., 2022a; Song et al., 2023; Luo et al., 2023; Sauer et al.,
2023; Yin et al., 2024b;a; Kang et al., 2024). Another line of works choose to optimize or accelerate
computation via efficient architecture design (Li et al., 2023b; 2020; Cai et al., 2024; Liu et al., 2024a),
quantization (Shang et al., 2023; Li et al., 2023a), sparse inference (Li et al., 2022; Ma et al., 2024b;a),
and distributed inference (Li et al., 2024b; Wang et al., 2024c; Chen et al., 2024b). This work focuses
on quantizing the diffusion models to 4 bits to reduce the computation complexity. Our method can
also be applied to the few-step diffusion models to further reduce the latency (see Section 5.2).
Quantization. Quantization has been recognized as an effective approach for LLMs to reduce the
model size and accelerate inference (Dettmers et al., 2022; Frantar et al., 2023; Xiao et al., 2023; Lin
et al., 2024b;a; Kim et al., 2024; Zhao et al., 2024d). For diffusion models, Q-Diffusion (Li et al.,
2023a) and PTQ4DM (Shang et al., 2023) first achieved 8-bit quantization. Subsequent works refined
these techniques with approaches like sensitivity analysis (Yang et al., 2023) and timestep-aware
quantization (He et al., 2023; Huang et al., 2024; Liu et al., 2024b; Wang et al., 2024a). Some
recent works extended the setting to text-to-image models (Tang et al., 2023; Zhao et al., 2024c), DiT
backbones (Wu et al., 2024), quantization-aware training (He et al., 2024; Zheng et al., 2024; Wang
et al., 2024b; Sui et al., 2024), video generation (Zhao et al., 2024b), and different data types (Liu
& Zhang, 2024). Among these works, only MixDQ (Zhao et al., 2024c) and ViDiT-Q (Zhao et al.,
2024b) implement low-bit inference engines and report measured 8-bit speedup on GPUs. In this
work, we push the boundary further by quantizing diffusion models to 4 bits, supporting both the
integer or floating-point data types, compatible with the UNet backbone (Ho et al., 2020) and recent

3



SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models

DiT architecture (Peebles & Xie, 2023). Our co-designed inference engine, Nunchaku, further ensures
on-hardware speedup. Additionally, when applying LoRA to the model, existing methods require
fusing the LoRA branch to the main branch and re-quantizing the model to avoid tremendous memory-
access overhead in the LoRA branch. Nunchaku cuts off this overhead via kernel fusion, allowing the
low-rank branch to run efficiently as a separate branch, eliminating the need for re-quantization.
Low-rank decomposition. Low-rank decomposition has gained significant attention in deep learning
for enhancing computational and memory efficiency (Hu et al., 2022; Zhao et al., 2024a; Jaiswal
et al., 2024). While directly applying this approach to model weights can reduce the compute and
memory demands (Hsu et al., 2022; Yuan et al., 2023; Li et al., 2023c), it often leads to performance
degradation. Instead, Yao et al. (2023) combined it with quantization for model compression,
employing a low-rank branch to compensate for the quantization error. Low-Rank Adaptation
(LoRA) (Hu et al., 2022) introduces another important line of research by using low-rank matrices
to adjust a subset of pre-trained weights for efficient fine-tuning. This has sparked numerous
advancements (Dettmers et al., 2023; Guo et al., 2024; Li et al., 2024c; He et al., 2024; Xu et al.,
2024b), which combines quantized models with low-rank adapters to reduce memory usage during
model fine-tuning. However, our work differs in two major aspects. Firstly, our goal is different,
as we aim to accelerate model inference through quantization, while previous works focus on
model compression or efficient fine-tuning. Thus, they primarily consider weight-only quantization,
resulting in no speedup. Secondly, as shown in our experiments (Figure 6 and ablation study in
Section 5.2), directly applying these methods not only degrades the image quality, but also introduces
significant overhead. In contrast, our method yields much better performance due to our joint
quantization of weights and activations and our inference engine Nunchaku minimizes the overhead
by fusing the low-rank branch kernels into the low-bit computation ones.

3 QUANTIZATION PRELIMINARY

Quantization is an effective approach to accelerate linear layers in networks. Given a tensor X , the
quantization process is defined as:

QX = round
(
X

sX

)
, sX =

max(|X|)
qmax

. (1)

Here, QX is the low-bit representation of X , sX is the scaling factor, and qmax is the maximum
quantized value. For signed b-bit integer quantization, qmax = 2b−1 − 1. For 4-bit floating-point
quantization with 1-bit mantissa and 2-bit exponent, qmax = 6. Thus, the dequantized tensor can be
formulated as Q(X) = sX ·QX . For a linear layer with input X and weight W , its computation
can be approximated by

XW ≈ Q(X)Q(W ) = sXsW ·QXQW . (2)

The same approximation applies to convolutional layers. To speed up computation, modern arithmetic
logic units require both QX and QW using the same bit width. Otherwise, the low-bit side needs
to be upcast to match the higher bit width, negating the speed advantage. Following the notation in
QServe (Lin et al., 2024b), we denote x-bit weight, y-bit activation as WxAy. “INT” and “FP” refer
to the integer and floating-point data types, respectively.

In this work, we focus on W4A4 quantization for acceleration, where outliers in both weights
and activations place substantial obstacles. Traditional methods to suppress these outliers include
quantization-aware training (QAT) (He et al., 2024) and rotation (Ashkboos et al., 2024; Liu et al.,
2024c; Lin et al., 2024b). QAT requires massive computing resources, especially for tuning models
with more than 10B parameters (e.g., FLUX.1). Rotation is inapplicable due to the usage of adaptive
normalization layers (Peebles & Xie, 2023) in diffusion models. The runtime-generated normalization
weights preclude the offline integration of the rotation matrix with the weights of projection layers.
Consequently, online rotation of both activations and weights incurs significant runtime overhead.

4 METHOD

In this section, we first formulate our problem and discuss where the quantization error comes from.
Next, we present SVDQuant, a new W4A4 quantization paradigm for diffusion models. Our key idea

4



SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models

0 7k 14k 21k
0

0.5

1

1.5

2

0 7k 14k 21k
0

5

10

15

20

0 18k 55k 74k
0

0.2

0.4

0.8

1.0

0.6

37k0 7k 14k 21k
0

0.5

1.0

1.5

2.0

0 18k 37k 55k 74k
0

4

6

10

2

8

Input Activation Group Index Weight Group Index Weight Group IndexWeight Group IndexInput Activation Group Index

W
ei

gh
t V

al
ue

In
pu

t A
ct

iv
at

io
n 

Va
lu

e

W
ei

gh
t V

al
ue

W
ei

gh
t V

al
ue

In
pu

t A
ct

iv
at

io
n 

Va
lu

e

(a) |X | (b) |W | (c) | X̂ | = |X ⋅ diag(λ)−1 | (d) |Ŵ | = |W ⋅ diag(λ) | (e) |R | = |Ŵ − L1L2 |

50% Percentile

99% Percentile

Max
Original After Smoothing After SVDOutliers

Outliers
More 

Outliers

Figure 4: Example value distribution of inputs and weights in PixArt-Σ. λ is the smooth factor. Red indicates
the outliers. Initially, both the input X and weight W contain significant outliers. After smoothing, the range of
X̂ is reduced with much fewer outliers, while Ŵ shows more outliers. Once the SVD low-rank branch L1L2 is
subtracted, the residual R has a narrower range and is free from outliers.

is to introduce an additional low-rank branch that can absorb quantization difficulties in both weights
and activations. Finally, we provide a co-designed inference engine Nunchaku with kernel fusion to
minimize the overhead of the low-rank branches in the 4-bit model.

4.1 PROBLEM FORMULATION

Consider a linear layer with input X ∈ Rb×m and weight W ∈ Rm×n. The quantization error can
be defined as

E(X,W ) = ∥XW −Q(X)Q(W )∥F , (3)

where ∥ · ∥F denotes Frobenius Norm.

Proposition 4.1 (Error decomposition). The quantization error can be decomposed as follows:

E(X,W ) ≤ ∥X∥F ∥W −Q(W )∥F + ∥X −Q(X)∥F (∥W ∥F + ∥W −Q(W )∥F ). (4)

See Appendix A.1 for the proof. From the proposition, we can see that the error is bounded by
four elements – the magnitude of the weight and input, ∥W ∥F and ∥X∥F , and their respective
quantization errors, ∥W −Q(W )∥F and ∥X −Q(X)∥F . To minimize the overall quantization
error, we aim to optimize these four terms.

4.2 SVDQUANT: ABSORBING OUTLIERS VIA LOW-RANK BRANCH

Migrate outliers from activation to weight. Smoothing (Xiao et al., 2023; Lin et al., 2024a) is an
effective approach for reducing outliers. We can smooth outliers in activations by scaling down the
input X and adjusting the weight matrix W correspondingly using a per-channel smoothing factor
λ ∈ Rm. As shown in Figure 4(a)(c), the smoothed input X̂ = X · diag(λ)−1 exhibits reduced
magnitude and fewer outliers, resulting in lower input quantization error. However, in Figure 4(b)(d),
the transformed weight Ŵ = W · diag(λ) has a significant increase in both magnitude and the
presence of outliers, which in turn raises the weight quantization error. Consequently, the overall
error reduction is limited.
Absorb magnified weight outliers with a low-rank branch. Our core insight is to introduce a 16-bit
low-rank branch and further migrate the weight quantization difficulty to this branch. Specifically,
we decompose the transformed weight as Ŵ = L1L2 +R, where L1 ∈ Rm×r and L2 ∈ Rr×n are
two low-rank factors of rank r, and R is the residual. Then XW can be approximated as

XW = X̂Ŵ = X̂L1L2 + X̂R ≈ X̂L1L2︸ ︷︷ ︸
16-bit low-rank branch

+Q(X̂)Q(R)︸ ︷︷ ︸
4-bit residual

. (5)

Compared to direct 4-bit quantization, i.e., Q(X̂)Q(W ), our method first computes the low-rank
branch X̂L1L2 in 16-bit precision, and then approximates the residual X̂R with 4-bit quantization.
Empirically, r ≪ min(m,n), and is typically set to 16 or 32. As a result, the additional parameters
and computation for the low-rank branch are negligible, contributing only mr+nr

mn to the overall costs.
However, it still requires careful system design to eliminate redundant memory access, which we will
discuss in Section 4.3.

5



SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models

La
te

nc
y 

(u
s)

0

100

200

300

400

500

Naïve Nunchaku (Ours)

Quantize
4-bit Compute
Up Proj.
Down Proj.

1.43 ×

(a) Latency Breakdown on QKV projection

Fused Kernel 1

X̂
4-Bit Compute 

 QR, s
RQX̂, s

X̂

Quantize

Down Proj. 
 L1

Up Proj. 
 L2

X̂L1
X̂L1L2

⊕
s

X̂
s

R
QX̂QR

(b) Nunchaku Kernel Fusion

Fused Kernel 2

Shared OutputShared Input

7

287

150

17

300

22

Figure 6: (a) Naïvely running low-rank branch with rank 32 will introduce 57% latency overhead due to extra
read of 16-bit inputs in Down Projection and extra write of 16-bit outputs in Up Projection. Our Nunchaku
engine optimizes this overhead with kernel fusion. (b) Down Projection and Quantize kernels use the same input,
while Up Projection and 4-Bit Compute kernels share the same output. To reduce data movement overhead, we
fuse the first two and the latter two kernels together.

From Equation 5, the quantization error can be expressed as∥∥∥X̂Ŵ − (X̂L1L2 +Q(X̂)Q(R))
∥∥∥
F
=

∥∥∥X̂R−Q(X̂)Q(R)
∥∥∥
F
= E(X̂,R), (6)

where R = Ŵ −L1L2. According to Proposition 4.1, since X̂ is already free from outliers, we only
need to focus on optimizing the magnitude of R, ∥R∥F and its quantization error, ∥R−Q(R)∥F .
Proposition 4.2 (Quantization error bound). For any tensor R and quantization method described in
Equation 1 as Q(R) = sR ·QR. Assuming the element of R follows a normal distribution, we have

E [∥R−Q(R)∥F ] ≤
√
log (size(R))π

qmax
E [∥R∥F ] , (7)

where size(R) denotes the number of elements in R.

0 16 32 48 64

100

400

10

 W
 Ŵ
 R

Extremely 
High 
Singular 
Values Low 

Singular 
Values

Figure 5: First 64 singular values
of W , Ŵ , and R. The first 32
singular values of Ŵ exhibit a
steep drop, while the remaining
values are much more gradual.

See Appendix A.2 for the proof. From this proposition, we obtain
the intuition that the quantization error ∥R−Q(R)∥F is bounded
by the magnitude of the residual ∥R∥F . Thus, our goal is to

find the optimal L1L2 that minimizes ∥R∥F =
∥∥∥Ŵ −L1L2

∥∥∥
F

,
which can be solved by simple Singular Value Decomposition
(SVD). Given the SVD of Ŵ = UΣV , the optimal solution is
L1 = UΣ:,:r and L2 = V:r,:. Figure 5 illustrates the singular
value distribution of the original weight W , transformed weight
Ŵ and residual R. The singular values of the original weight W
are highly imbalanced. After smoothing, the singular value dis-
tribution of Ŵ becomes even sharper, with only the first several
values being significantly larger. By removing these dominant val-
ues, Eckart–Young–Mirsky theorem§ suggests that the magnitude

of the residual R is dramatically reduced, as ∥R∥F =
√∑min(m,n)

i=r+1 σ2
i , compared to the original

magnitude
∥∥∥Ŵ∥∥∥

F
=

√∑min(m,n)
i=1 σ2

i , where σi is the i-th singular value of Ŵ . Furthermore,
empirical observations reveal that R exhibits fewer outliers with a substantially compressed value
range compared to Ŵ , as shown in Figure 4(d)(e). In practice, we can further reduce quantization
errors by iteratively updating the low-rank branch through decomposing W −Q(R) and adjusting
R accordingly for several iterations, and then picking the result with the smallest error.

4.3 NUNCHAKU: FUSING LOW-RANK AND LOW-BIT BRANCH KERNELS

Although the low-rank branch introduces theoretically negligible computation, running it as a separate
branch would incur significant latency overhead—approximately 50% of the 4-bit branch latency,
as shown in Figure 6(a). This is because, for a small rank r, even though the computational cost
is greatly reduced, the data sizes of input and output activations remain unchanged, shifting the

§https://en.wikipedia.org/wiki/Low-rank_approximation

6

https://en.wikipedia.org/wiki/Low-rank_approximation


SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models

bottleneck from computation to memory access. The situation deteriorates, especially when the
activation cannot fit into the GPU L2 cache. For example, the up projection in the low-rank branch
for QKV projection is much slower since its output exceeds the available L2 cache and results in
the extra load and store operations to DRAM. Fortunately, we observe that the down projection
L1 in the low-rank branch shares the same input as the quantization kernel in the low-bit branch,
while the up projection L2 shares the same output as the 4-bit computation kernel, as illustrated in
Figure 6(b). By fusing the down projection with the quantization kernel and the up projection with
the 4-bit computation kernel, the low-rank branch can share the activations with the low-bit branch,
eliminating the extra memory access and also halving the number of kernel calls. As a result, the
low-rank branch adds only 5∼10% latency, making it nearly cost-free.

5 EXPERIMENTS

5.1 SETUPS

Models. We benchmark our methods using the following text-to-image models, including both the
UNet (Ronneberger et al., 2015; Ho et al., 2020) and DiT (Peebles & Xie, 2023) backbones:

• FLUX.1 (Black-Forest-Labs, 2024) is the SoTA open-sourced DiT-based diffusion model. It
consists of 19 joint attention blocks (Esser et al., 2024) and 38 parallel attention blocks (Dehghani
et al., 2023), totaling 12B parameters. We evaluate on both the 50-step guidance-distilled (FLUX.1-
dev) and 4-step timestep-distilled (FLUX.1-schnell) variants.

• PixArt-Σ (Chen et al., 2024a) is another DiT-based model. Instead of using joint attention, it stacks
28 attention blocks composed of self-attention, cross-attention, and feed-forward layers, amounting
to 600M parameters. We evaluate it on the default 20-step setting.

• Stable Diffusion XL (SDXL) is a widely-used UNet-based model with 2.6B parameters (Podell
et al., 2024). It predicts noise with three resolution scales. The highest-resolution stage is processed
entirely by ResBlocks (He et al., 2016), while the other two stages jointly use ResBlocks and atten-
tion layers. Like PixArt-Σ, SDXL employs cross-attention layers for text conditioning. We evaluate
it in the 30-step setting, along with its 4-step distilled variant, SDXL-Turbo (Sauer et al., 2023).

Datasets. Following previous works (Li et al., 2023a; Zhao et al., 2024c;b), we randomly sample the
prompts in COCO Captions 2024 (Chen et al., 2015) for calibration. To assess the generalization
capability of our method, we adopt two distinct prompt sets with varying styles for benchmarking:

• MJHQ-30K (Li et al., 2024a) consists of 30K samples from Midjourney with 10 common categories,
3K samples each. We uniformly select 5K prompts from this dataset to evaluate model performance
on artistic image generation.

• Densely Captioned Images (DCI) (Urbanek et al., 2024) is a dataset containing ∼8K images with
detailed human-annotated captions, averaging over 1,000 words. For our experiments, we use
the summarized version (sDCI), where captions are condensed to 77 tokens using large language
models (LLMs) to accommodate diffusion models. Similarly, we randomly sample 5K prompts for
efficient evaluation of realistic image generation.

Baselines. We compare SVDQuant against the following post-training quantization (PTQ) methods:

• 4-bit NormalFloat (NF4) is a data type for weight-only quantization (Dettmers et al., 2023). It
assumes that weights follow a normal distribution and is the information-theoretically optimal
4-bit representation. We use the community-quantized NF4 FLUX.1 models (Lllyasviel, 2024)
as the baselines.

• ViDiT-Q (Zhao et al., 2024b) uses per-token quantization and smoothing (Xiao et al., 2023) to
alleviate the outliers across different batches and tokens and achieves lossless 8-bit quantization
on PixArt-Σ.

• MixDQ (Zhao et al., 2024c) identifies the outliers in the begin-of-sentence token of text embedding
and protects them with 16-bit pre-computation. This method enables up to W4A8 quantization
with negligible performance degradation on SDXL-Turbo.

• TensorRT contains an industry-level PTQ toolkit to quantize the diffusion models to 8 bits. It uses
smoothing and only calibrates activations over a selected timestep range with a percentile scheme.

7

https://developer.nvidia.com/blog/tensorrt-accelerates-stable-diffusion-nearly-2x-faster-with-8-bit-post-training-quantization/


SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models

Table 1: Quantitative quality comparisons across different models. IR means ImageReward. Our 8-bit results
closely match the quality of the 16-bit models. Moreover, our 4-bit results outperform other 4-bit baselines,
effectively preserving the visual quality of 16-bit models.

MJHQ sDCI

Backbone Model Precision Method Quality Similarity Quality Similarity

FID (↓) IR (↑) LPIPS (↓) PSNR( ↑) FID (↓) IR (↑) LPIPS (↓) PSNR (↑)

FLUX.1
-dev

(50 Steps)

BF16 – 20.3 0.953 – – 24.8 1.02 – –

INT W8A8 Ours 20.4 0.948 0.089 27.0 24.7 1.02 0.106 24.9

W4A16 NF4 20.6 0.910 0.272 19.5 24.9 0.986 0.292 18.2
INT W4A4 Ours 19.9 0.932 0.254 20.1 24.7 0.992 0.273 18.8
FP W4A4 Ours 21.0 0.933 0.247 20.2 25.7 0.995 0.267 18.7

FLUX.1
-schnell
(4 Steps)

BF16 – 19.2 0.938 – – 20.8 0.932 – –

INT W8A8 Ours 19.2 0.966 0.120 22.9 20.7 0.975 0.133 21.3

DiT W4A16 NF4 18.9 0.943 0.257 18.2 20.7 0.953 0.263 17.1
INT W4A4 Ours 18.4 0.969 0.292 17.5 20.1 0.988 0.299 16.3
FP W4A4 Ours 19.9 0.956 0.279 17.5 21.5 0.967 0.278 16.6

PixArt-Σ
(20 Steps)

FP16 – 16.6 0.944 – – 24.8 0.966

INT W8A8 ViDiT-Q 15.7 0.944 0.137 22.5 23.5 0.974 0.163 20.4
INT W8A8 Ours 16.3 0.955 0.109 23.7 24.2 0.969 0.129 21.8

INT W4A8 ViDiT-Q 37.3 0.573 0.611 12.0 40.6 0.600 0.629 11.2
INT W4A4 ViDiT-Q 412 -2.27 0.854 6.44 425 -2.28 0.838 6.70
INT W4A4 Ours 20.1 0.898 0.394 16.2 25.1 0.922 0.434 14.9
FP W4A4 Ours 18.3 0.946 0.326 17.4 23.7 0.978 0.357 16.1

UNet

SDXL
-Turbo

(4 Steps)

FP16 – 24.3 0.845 – – 24.7 0.705 – –

INT W8A8 MixDQ 24.1 0.834 0.147 21.7 25.0 0.690 0.157 21.6
INT W8A8 Ours 24.3 0.845 0.100 24.0 24.8 0.701 0.110 23.7

INT W4A8 MixDQ 27.7 0.708 0.402 15.7 25.9 0.610 0.415 15.7
INT W4A4 MixDQ 353 -2.26 0.685 11.0 373 -2.28 0.686 11.3
INT W4A4 Ours 24.5 0.816 0.265 17.9 25.7 0.667 0.278 17.8
FP W4A4 Ours 24.1 0.822 0.250 18.5 24.7 0.699 0.261 18.4

SDXL
(30 Steps)

FP16 – 16.6 0.729 – – 22.5 0.573 – –

INT W8A8 TensorRT 20.2 0.591 0.247 22.0 25.4 0.453 0.265 21.7
INT W8A8 Ours 16.6 0.718 0.119 26.4 22.4 0.574 0.129 25.9

INT W4A4 Ours 20.7 0.609 0.298 20.6 26.3 0.494 0.314 20.4
FP W4A4 Ours 19.0 0.607 0.294 21.0 25.4 0.480 0.312 20.7

Metrics. Following previous work (Li et al., 2022; 2024b), we mainly benchmark image quality and
similarity to the results produced by the original 16-bit models. For the image quality assessment, we
use Fréchet Inception Distance (FID, lower is better) to measure the distribution distance between
the generated images and the ground-truth images (Heusel et al., 2017). Besides, we employ Image
Reward (higher is better) to approximate the human rating of the generated images (Xu et al., 2024a).
We use LPIPS (lower is better) to measure the perceptual similarity (Zhang et al., 2018) and Peak
Signal Noise Ratio (PSNR, higher is better) to measure the numerical similarity of the images from
the 16-bit models. Please refer to our Appendix B.1 for more metrics (CLIP IQA (Wang et al., 2023),
CLIP Score (Hessel et al., 2021) and SSIM¶).
Implementation details. For the 8-bit setting, we use per-token dynamic activation quantization and
per-channel weight quantization with a low-rank branch of rank 16. For the 4-bit setting, we adopt per-
group symmetric quantization for both activations and weights, along with a low-rank branch of rank
32. INT4 quantization uses a group size of 64 with 16-bit scales. FP4 quantization uses a group size of
32 with FP8 scales (Rouhani et al., 2023). For FLUX.1 models, the inputs of linear layers in adaptive
normalization are kept in 16 bits (i.e., W4A16). For other models, key and value projections in the
cross-attention are retained at 16 bits since their latency only covers less than 5% of total runtime.

5.2 RESULTS

Quality results. We report the quantitative quality results in Table 1 across various models and
precision levels, and show some corresponding 4-bit qualitative comparisons in Figure 7. Among

¶https://en.wikipedia.org/wiki/Structural_similarity_index_measure

8

https://en.wikipedia.org/wiki/Structural_similarity_index_measure


SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models

Prompt: Night time, a bar with a dog outside, the bar is made of cargo container boxes and the 
logo outside the container is neon logo that says Bark and Brew

Prompt: a fasion model is wearing the colorful clothes, fantasic,fasionable, pop,dinosaur style

Prompt: man pouring coffee into a cup, but with a unique twist the stream of coffee bends and curves 
to fill the cup perfectly.take with Leica, 35mm, ISO 100, softfocus, Cinematic Lightning, 

hyperdetailed, full HD

FLUX.1-dev BF16 
Image Reward: 0.953

NF4 W4A16 
Image Reward: 0.910

Our INT W4A4 
Image Reward: 0.924

Our FP W4A4 
Image Reward: 0.932

FLUX.1-schnell BF16 
Image Reward: 0.968

NF4 W4A16 
Image Reward: 0.943

Our INT W4A4 
Image Reward: 0.965

Our FP W4A4 
Image Reward: 0.957

Prompt: A smiling woman planting tomato seedlings in her permaculture garden, sunny day, a 
greenhouse in the background, retro modern styling, highly realistic with a cinematic background blur, 
Focal point and angle evoking a filmic perspective, Photography, DSLR with a 35mm prime lens at f2.8

Prompt: cute front of restaurant, winter

PixArt-  FP16 
Image Reward: 0.944

Σ ViDiT-Q INT W4A8 
Image Reward: 0.573

Our INT W4A4 
Image Reward: 0.898

Our FP W4A4 
Image Reward: 0.946

Prompt: illustration for 3 happy cute girls, one with curly hair, second with wavy hair, 
the third with striaght hair ,8k

Prompt: hummingbird flying near a flower. 4k ultra realistic ray tracing dynamic lighting

SDXL-Turbo FP16 
Image Reward: 0.845

MixDQ INT W4A8 
Image Reward: 0.708

Our INT W4A4 
Image Reward: 0.796

Our FP W4A4 
Image Reward: 0.822

Prompt: Close up portrait deep underwater ,light, epic, green jungle ,flower white, 
fox red, detailed, pretty face, dark background, detailed, photo

Prompt: logo of the number 30 made of colorful different sizes rectangles, neumorphism, 
white background, bright lights, sharp focus

Prompt: Victorain Vintage girl in a green dress sitting under a tree reading a book

Prompt: a realistic portrait of taylor swift with a red scarf

(c) PixArt- Σ (d) SDXL-Turbo

(a) FLUX.1-dev (b) FLUX.1-schnell

Prompt: A white Havanese dog in sunglasses riding a motorcycle

Figure 7: Qualitative visual results on MJHQ. Image Reward is calculated over the entire dataset. On FLUX.1
models, our 4-bit models outperform the NF4 W4A16 baselines, demonstrating superior text alignment and
closer similarity to the 16-bit models. For instance, NF4 misinterprets “dinosaur style,” generating a real
dinosaur. On PixArt-Σ and SDXL-Turbo, our 4-bit results demonstrate noticeably better visual quality than
ViDiT-Q’s and MixDQ’s W4A8 results.

all models, our 8-bit results can perfectly mirror the 16-bit results, achieving PSNR higher than 21,
beating all other 8-bit baselines. On FLUX.1-dev, our INT8 PSNR even reaches 27 on MJHQ.

For 4-bit quantization, on FLUX.1, our SVDQuant surpasses the NF4 W4A16 baseline regarding
Image Reward. On the schnell variant, our Image Reward even exceeds that of the original BF16
model, suggesting stronger human preference. On PixArt-Σ, while our INT4 Image Reward shows
slight degradation, our FP4 model achieves an even higher score than the FP16 model. This is
likely due to PixArt-Σ’s small model size (600M parameters), which is already highly compact
and benefits from a smaller group size. Remarkably, both our INT4 and FP4 results consistently
outperform ViDiT-Q’s|| W4A8 results by a large margin across all metrics. For UNet-based models,
on SDXL-Turbo, our 4-bit models significantly outperform MixDQ’s W4A8 results, and our FID
scores are on par with the FP16 models, indicating no loss in performance. On SDXL, both our INT4
and FP4 results achieve comparable quality to TensorRT’s W8A8 performance, which represents the
8-bit SoTA. As shown in Figure 15 in the Appendix, our visual quality only shows minor degradation.

||Our FP16 PixArt-Σ model is slightly different from ViDiT’s, though both offer the same quality. For fair
comparisons, ViDiT-Q’s similarity results are calculated using their FP16 results.

9



SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models

0

175

350

525

700

0

500

1000

1500

2000

0

6

12

18

24
BF16 W4A16 - NF4 W4A4 - INT4

0

6

12

18

24

(b) DiT Inference Memory (GiB) (c) Single Step Latency 
on Desktop 4090 (ms)(a) Model Size (GiB)

3.0 ×
657 660

218

3.5 ×

6.9 6.5

22.7

5.8 6.1

22.2
3.8 × 3.6 × 3.3 ×

4382

1320

433
(d) Single Step Latency 

on Laptop 4090 (ms)

3.3 ×
10.1 ×

Figure 8: SVDQuant reduces the model size of the 12B FLUX.1 by 3.6×. Additionally, our engine, Nunchaku,
further cuts memory usage of the 16-bit model by 3.5× and delivers 3.0× speedups over the NF4 W4A16 baseline
on both the desktop and laptop NVIDIA RTX 4090 GPUs. Remarkably, on laptop 4090, it achieves in total
10.1× speedup by eliminating CPU offloading.

Realism LoRA Ghibsky Illustration LoRA Anime LoRA Children Sketch LoRA Yarn Art LoRA

FLUX.1-dev 
BF16

Our INT4

Figure 9: Our INT4 model seamlessly integrates with off-the-shelf LoRAs without requiring requantization.
When applying LoRAs, it matches the image quality of the original 16-bit FLUX.1-dev. See Appendix C for
the text prompts.

Memory save & speedup. In Figure 8, we report measured model size, memory savings, and speedup
for FLUX.1. Our INT4 quantization reduces the original transformer size from 22.2 GiB to 6.1 GiB,
including a 0.3 GiB overhead due to the low-rank branch, resulting in an overall 3.6× reduction. Since
both weights and activations are quantized, compared to the NF4 weight-only-quantized variant,
our inference engine Nunchaku even saves more memory footprint, and offers 3.0× speedup on
both desktop- and laptop-level NVIDIA RTX 4090 GPUs. Notably, while the original BF16 model
requires per-layer CPU offloading on the 16GB laptop 4090, our INT4 model fits entirely in GPU
memory, resulting in a 10.1× speedup by avoiding offloading. We anticipate even greater speedups
for FP4-quantized models on NVIDIA’s next-generation Blackwell GPUs, as they inherently support
microscaling for group quantization without the need for specialized GPU kernels.
Integrate with LoRA. Previous quantization methods require fusing the LoRA branches and re-
quantizing the model when integrating LoRAs. In contrast, our Nunchaku eliminates redundant
memory access, allowing adding a separate LoRA branch. In practice, we can fuse the LoRA branch
into our low-rank branch by slightly increasing the rank, further enhancing efficiency. In Figure 9,
we exhibit some visual examples of applying LoRAs of five different styles (Realism, Ghibsky
Illustration, Anime, Children Sketch, and Yarn Art) to our INT4 FLUX.1-dev model. Our INT4
model successfully adapts to each style while preserving the image quality of the 16-bit version. For
more visual examples, see Appendix B.2.
Ablation study. In Figure 10, we present several ablation studies of SVDQuant on PixArt-Σ. First,
both SVD-only and naïve quantization perform poorly in the 4-bit setting, resulting in a severe degra-
dation of image quality. While applying smoothing to the quantization slightly improves image quality
compared to naïve quantization, the overall results remain unsatisfactory. LoRC (Yao et al., 2023)
introduces a low-rank branch to compensate for quantization errors, but this approach is suboptimal.
Quantization errors exhibit a smooth singular value distribution. Consequently, low-rank compensa-
tion fails to effectively mitigate these errors, as discussed in Section 4.2. In contrast, we first decom-
pose the weights and quantize only the residual. As demonstrated in Figure 5, the first several singular
values are significantly larger than the rest, allowing us to shift them to the low-rank branch, which

10

https://huggingface.co/XLabs-AI/flux-RealismLora
https://huggingface.co/aleksa-codes/flux-ghibsky-illustration
https://huggingface.co/aleksa-codes/flux-ghibsky-illustration
https://huggingface.co/alvdansen/sonny-anime-fixed
https://huggingface.co/Shakker-Labs/FLUX.1-dev-LoRA-Children-Simple-Sketch
https://huggingface.co/linoyts/yarn_art_Flux_LoRA


SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models

Prompt: beach stock image popular no text prompt trend. pinterest contest winner

Prompt: recipe image, angry crab sallad, in salvador dali style photographed by david lachapelle, eerie,  
rennaisance colors, award winning recipe on white background

PixArt- : FP16 
Image Reward: 0.931

Σ SVD Only 
Image Reward: -2.18

Naïve Quantization 
Image Reward: -1.12

Smoothing 
Image Reward: 0.508

Ours w/o Smoothing 
Image Reward: 0.690

Ours 
Image Reward: 0.858

LoRC 
Image Reward: -0.965

Figure 10: Ablation study of SVDQuant on PixArt-Σ. The rank of the low-rank branch is 64. Image Reward is
measured over 1K samples from MJHQ. Our results significantly outperform the others, achieving the highest
image quality by a wide margin.

Prompt: award winning photography of a beautiful medic smiling

PixArt- : FP16Σ SVDQuant Rank=16 
Image Reward: 0.787

SVDQuant Rank=32 
Image Reward: 0.829

SVDQuant Rank=64 
Image Reward: 0.859

0%

3%

6%

9%

12%
11.3%

5.6%

2.8%

Rank=16 Rank=32 Rank=64

0.0%

2.5%

5.0%

7.5%

10.0%

8.8%

5.2%

3.3%

M
od

el
 S

iz
e 

O
ve

rh
ea

d

La
te

nc
y 

O
ve

rh
ea

d

Figure 11: Increasing the rank r of the low-rank branch in SVDQuant can enhance image quality, but it also
leads to higher parameter and latency overhead.

effectively reduces weight magnitude. Finally, smoothing consolidate the outliers, further enabling
the low-rank branch to absorb outliers from the activations and substantially improving image quality.
Trade-off of increasing rank. Figure 11 presents the results of different rank r in SVDQuant on
PixArt-Σ. Increasing the rank from 16 to 64 significantly enhances image quality but increases
parameter and latency overhead. In our experiments, we select a rank of 32, which offers a decent
quality with minor overhead.

6 CONCLUSION & DISCUSSION

In this work, we introduce a novel 4-bit post-training quantization paradigm, SVDQuant, for diffusion
models. It adopts a low-rank branch to absorb the outliers in both the weights and activations, easing
the process of quantization. Our inference engine Nunchaku further fuses the low-rank and low-bit
branch kernels, reducing memory usage and cutting off redundant data movement overhead. Extensive
experiments demonstrate that SVDQuant preserves image quality. Nunchaku further achieves a 3.5×
reduction in memory usage over the original 16-bit model and 3.0× speedup over the weight-only quan-
tization on an NVIDIA RTX-4090 laptop. This advancement enables the efficient deployment of large-
scale diffusion models on edge devices, unlocking broader potential for interactive AI applications.
Limitations. In this work, we do not report the speedups for our FP4 models. This is because we
have no access to Blackwell GPUs, which natively support the precision and microscaling for group
quantization. On Blackwell hardware, we anticipate greater speedups compared to our INT4 results
on 4090 GPUs.

ACKNOWLEDGMENTS

We thank MIT-IBM Watson AI Lab, MIT and Amazon Science Hub, MIT AI Hardware Program,
National Science Foundation, Packard Foundation, Dell, LG, Hyundai, and Samsung for supporting
this research. We thank NVIDIA for donating the DGX server.

11



SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models

REFERENCES

Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian L Croci, Bo Li, Martin Jaggi, Dan Alistarh,
Torsten Hoefler, and James Hensman. Quarot: Outlier-free 4-bit inference in rotated llms. arXiv
preprint arXiv:2404.00456, 2024. 4

Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala,
Timo Aila, Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image diffusion models with an
ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022. 3

Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth
words: A vit backbone for diffusion models. In CVPR, 2023. 3

Black-Forest-Labs. Flux.1, 2024. URL https://blackforestlabs.ai/. 2, 7

Han Cai, Muyang Li, Qinsheng Zhang, Ming-Yu Liu, and Song Han. Condition-aware neural network
for controlled image generation. In CVPR, 2024. 3

Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping
Luo, Huchuan Lu, and Zhenguo Li. Pixart-sigma: Weak-to-strong training of diffusion transformer
for 4k text-to-image generation. arXiv preprint arXiv:2403.04692, 2024a. 7

Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, and
C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint
arXiv:1504.00325, 2015. 7

Zigeng Chen, Xinyin Ma, Gongfan Fang, Zhenxiong Tan, and Xinchao Wang. Asyncdiff: Parallelizing
diffusion models by asynchronous denoising. arXiv preprint arXiv:2406.06911, 2024b. 3

Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer,
Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Scaling
vision transformers to 22 billion parameters. In ICML. PMLR, 2023. 7

Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Gpt3. int8 (): 8-bit matrix
multiplication for transformers at scale. NeurIPS, 2022. 2, 3

Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. QLoRA: Efficient finetuning
of quantized LLMs. In Thirty-seventh Conference on Neural Information Processing Systems,
2023. 4, 7

Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam
Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for
high-resolution image synthesis. In ICML, 2024. 3, 7

fal.ai. Auraflow v0.1, 2024. URL https://blog.fal.ai/auraflow/. 2

Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. GPTQ: Accurate post-training
compression for generative pretrained transformers. ICLR, 2023. 3

Han Guo, Philip Greengard, Eric Xing, and Yoon Kim. Lq-lora: Low-rank plus quantized matrix
decomposition for efficient language model finetuning. ICLR, 2024. 4

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In CVPR, 2016. 7

Yefei He, Luping Liu, Jing Liu, Weijia Wu, Hong Zhou, and Bohan Zhuang. Ptqd: Accurate
post-training quantization for diffusion models. NeurIPS, 2023. 3

Yefei He, Jing Liu, Weijia Wu, Hong Zhou, and Bohan Zhuang. Efficientdm: Efficient quantization-
aware fine-tuning of low-bit diffusion models. In ICLR, 2024. 3, 4

Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A reference-
free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021. 8, 18

12

https://blackforestlabs.ai/
https://blog.fal.ai/auraflow/


SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models

Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans
trained by a two time-scale update rule converge to a local nash equilibrium. NeurIPS, 2017. 8

Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, 2020.
2, 3, 7

Yen-Chang Hsu, Ting Hua, Sungen Chang, Qian Lou, Yilin Shen, and Hongxia Jin. Language model
compression with weighted low-rank factorization. In The Tenth International Conference on
Learning Representations, 2022. 3, 4

Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
and Weizhu Chen. Lora: Low-rank adaptation of large language models. In The Tenth International
Conference on Learning Representations, 2022. 4

Yushi Huang, Ruihao Gong, Jing Liu, Tianlong Chen, and Xianglong Liu. Tfmq-dm: Temporal
feature maintenance quantization for diffusion models. In CVPR, 2024. 3

Ajay Jaiswal, Lu Yin, Zhenyu Zhang, Shiwei Liu, Jiawei Zhao, Yuandong Tian, and Zhangyang Wang.
From galore to welore: How low-rank weights non-uniformly emerge from low-rank gradients.
arXiv preprint arXiv: 2407.11239, 2024. 4

Minguk Kang, Richard Zhang, Connelly Barnes, Sylvain Paris, Suha Kwak, Jaesik Park, Eli Shecht-
man, Jun-Yan Zhu, and Taesung Park. Distilling diffusion models into conditional gans. arXiv
preprint arXiv:2405.05967, 2024. 3

Sehoon Kim, Coleman Richard Charles Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen,
Michael W. Mahoney, and Kurt Keutzer. SqueezeLLM: Dense-and-sparse quantization. In ICML,
2024. 3

Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi. Playground
v2.5: Three insights towards enhancing aesthetic quality in text-to-image generation, 2024a. 7

Muyang Li, Ji Lin, Yaoyao Ding, Zhijian Liu, Jun-Yan Zhu, and Song Han. Gan compression:
Efficient architectures for interactive conditional gans. In CVPR, 2020. 3

Muyang Li, Ji Lin, Chenlin Meng, Stefano Ermon, Song Han, and Jun-Yan Zhu. Efficient spatially
sparse inference for conditional gans and diffusion models. In NeurIPS, 2022. 3, 8

Muyang Li, Tianle Cai, Jiaxin Cao, Qinsheng Zhang, Han Cai, Junjie Bai, Yangqing Jia, Ming-Yu Liu,
Kai Li, and Song Han. Distrifusion: Distributed parallel inference for high-resolution diffusion
models. In CVPR, 2024b. 3, 8

Xiuyu Li, Yijiang Liu, Long Lian, Huanrui Yang, Zhen Dong, Daniel Kang, Shanghang Zhang, and
Kurt Keutzer. Q-diffusion: Quantizing diffusion models. In ICCV, 2023a. 3, 7

Yanyu Li, Huan Wang, Qing Jin, Ju Hu, Pavlo Chemerys, Yun Fu, Yanzhi Wang, Sergey Tulyakov,
and Jian Ren. Snapfusion: Text-to-image diffusion model on mobile devices within two seconds.
NeurIPS, 2023b. 3

Yixiao Li, Yifan Yu, Qingru Zhang, Chen Liang, Pengcheng He, Weizhu Chen, and Tuo Zhao.
LoSparse: Structured compression of large language models based on low-rank and sparse approx-
imation. In Proceedings of the 40th International Conference on Machine Learning, volume 202,
pp. 20336–20350. PMLR, 2023c. 4

Yixiao Li, Yifan Yu, Chen Liang, Nikos Karampatziakis, Pengcheng He, Weizhu Chen, and Tuo
Zhao. Loftq: Lora-fine-tuning-aware quantization for large language models. In The Twelfth
International Conference on Learning Representations, 2024c. 4

Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan
Xiao, Xingyu Dang, Chuang Gan, and Song Han. Awq: Activation-aware weight quantization for
on-device llm compression and acceleration. In MLSys, 2024a. 2, 3, 5

Yujun Lin, Haotian Tang, Shang Yang, Zhekai Zhang, Guangxuan Xiao, Chuang Gan, and Song Han.
Qserve: W4a8kv4 quantization and system co-design for efficient llm serving. arXiv preprint
arXiv:2405.04532, 2024b. 3, 4

13



SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models

Songhua Liu, Weihao Yu, Zhenxiong Tan, and Xinchao Wang. Linfusion: 1 gpu, 1 minute, 16k
image. arXiv preprint arXiv:2409.02097, 2024a. 3

Wenxuan Liu and Saiqian Zhang. Hq-dit: Efficient diffusion transformer with fp4 hybrid quantization.
arXiv preprint arXiv:2405.19751, 2024. 3

Xuewen Liu, Zhikai Li, Junrui Xiao, and Qingyi Gu. Enhanced distribution alignment for post-training
quantization of diffusion models. arXiv preprint arXiv:2401.04585, 2024b. 3

Zechun Liu, Changsheng Zhao, Igor Fedorov, Bilge Soran, Dhruv Choudhary, Raghuraman Krish-
namoorthi, Vikas Chandra, Yuandong Tian, and Tijmen Blankevoort. Spinquant–llm quantization
with learned rotations. arXiv preprint arXiv:2405.16406, 2024c. 4

Lllyasviel. [major update] bitsandbytes guidelines and flux · lllyasviel stable-diffusion-
webui-forge · discussion #981, 2024. URL https://github.com/lllyasviel/
stable-diffusion-webui-forge/discussions/981. 7

Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode
solver for diffusion probabilistic model sampling in around 10 steps. In NeurIPS, 2022. 3

Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models:
Synthesizing high-resolution images with few-step inference. arXiv preprint arXiv: 2310.04378,
2023. 3

Xinyin Ma, Gongfan Fang, Michael Bi Mi, and Xinchao Wang. Learning-to-cache: Accelerating
diffusion transformer via layer caching. arXiv preprint arXiv:2406.01733, 2024a. 3

Xinyin Ma, Gongfan Fang, and Xinchao Wang. Deepcache: Accelerating diffusion models for free.
In CVPR, 2024b. 3

Pascal Massart. Concentration inequalities and model selection: Ecole d’Eté de Probabilités de
Saint-Flour XXXIII-2003. Springer, 2007. 17

Chenlin Meng, Ruiqi Gao, Diederik P Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On
distillation of guided diffusion models. arXiv preprint arXiv:2210.03142, 2022a. 3

Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon.
SDEdit: Guided image synthesis and editing with stochastic differential equations. In ICLR, 2022b.
2

NVIDIA. Nvidia blackwell architecture technical brief, 2024. URL https://resources.
nvidia.com/en-us-blackwell-architecture. 2

William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. 3, 4,
7

Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe
Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image
synthesis. In ICLR, 2024. 2, 3, 7

Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
models from natural language supervision. In ICML, 2021. 18

Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-
resolution image synthesis with latent diffusion models. In CVPR, 2022. 2, 3

Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical
image segmentation. In Medical Image Computing and Computer-Assisted Intervention–MICCAI
2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III
18, pp. 234–241. Springer, 2015. 3, 7

Bita Darvish Rouhani, Ritchie Zhao, Ankit More, Mathew Hall, Alireza Khodamoradi, Summer
Deng, Dhruv Choudhary, Marius Cornea, Eric Dellinger, Kristof Denolf, et al. Microscaling data
formats for deep learning. arXiv preprint arXiv:2310.10537, 2023. 8

14

https://github.com/lllyasviel/stable-diffusion-webui-forge/discussions/981
https://github.com/lllyasviel/stable-diffusion-webui-forge/discussions/981
https://resources.nvidia.com/en-us-blackwell-architecture
https://resources.nvidia.com/en-us-blackwell-architecture


SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models

Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman.
Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In CVPR,
2023. 2

Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In
ICLR, 2021. 3

Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion
distillation. arXiv preprint arXiv:2311.17042, 2023. 3, 7

Yuzhang Shang, Zhihang Yuan, Bin Xie, Bingzhe Wu, and Yan Yan. Post-training quantization on
diffusion models. In CVPR, 2023. 3

Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised
learning using nonequilibrium thermodynamics. In ICML, 2015. 3

Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. In ICML, 2023.
3

Yang Sui, Yanyu Li, Anil Kag, Yerlan Idelbayev, Junli Cao, Ju Hu, Dhritiman Sagar, Bo Yuan, Sergey
Tulyakov, and Jian Ren. Bitsfusion: 1.99 bits weight quantization of diffusion model. arXiv
preprint arXiv:2406.04333, 2024. 3

Siao Tang, Xin Wang, Hong Chen, Chaoyu Guan, Zewen Wu, Yansong Tang, and Wenwu Zhu.
Post-training quantization with progressive calibration and activation relaxing for text-to-image
diffusion models. arXiv preprint arXiv:2311.06322, 2023. 3

Jack Urbanek, Florian Bordes, Pietro Astolfi, Mary Williamson, Vasu Sharma, and Adriana Romero-
Soriano. A picture is worth more than 77 text tokens: Evaluating clip-style models on dense
captions. In CVPR, 2024. 7

Changyuan Wang, Ziwei Wang, Xiuwei Xu, Yansong Tang, Jie Zhou, and Jiwen Lu. Towards accurate
post-training quantization for diffusion models. In CVPR, 2024a. 3

Haoxuan Wang, Yuzhang Shang, Zhihang Yuan, Junyi Wu, and Yan Yan. Quest: Low-bit diffusion
model quantization via efficient selective finetuning. arXiv preprint arXiv:2402.03666, 2024b. 3

Jiannan Wang, Jiarui Fang, Aoyu Li, and PengCheng Yang. Pipefusion: Displaced patch pipeline
parallelism for inference of diffusion transformer models. arXiv preprint arXiv:2405.14430, 2024c.
3

Jianyi Wang, Kelvin CK Chan, and Chen Change Loy. Exploring clip for assessing the look and feel
of images. In AAAI, 2023. 8, 18

Junyi Wu, Haoxuan Wang, Yuzhang Shang, Mubarak Shah, and Yan Yan. Ptq4dit: Post-training
quantization for diffusion transformers. arXiv preprint arXiv:2405.16005, 2024. 3

Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant:
Accurate and efficient post-training quantization for large language models. In ICML, 2023. 2, 3,
5, 7

Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong.
Imagereward: Learning and evaluating human preferences for text-to-image generation. NeurIPS,
2024a. 8

Yuhui Xu, Lingxi Xie, Xiaotao Gu, Xin Chen, Heng Chang, Hengheng Zhang, Zhengsu Chen,
Xiaopeng Zhang, and Qi Tian. Qa-lora: Quantization-aware low-rank adaptation of large language
models. In The Twelfth International Conference on Learning Representations, 2024b. 4

Yuewei Yang, Xiaoliang Dai, Jialiang Wang, Peizhao Zhang, and Hongbo Zhang. Efficient quantiza-
tion strategies for latent diffusion models. arXiv preprint arXiv:2312.05431, 2023. 3

Zhewei Yao, Xiaoxia Wu, Cheng Li, Stephen Youn, and Yuxiong He. Zeroquant-v2: Exploring
post-training quantization in llms from comprehensive study to low rank compensation. arXiv
preprint arXiv:2303.08302, 2023. 4, 10

15



SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models

Tianwei Yin, Michaël Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and
William T Freeman. Improved distribution matching distillation for fast image synthesis. arXiv
preprint arXiv:2405.14867, 2024a. 3

Tianwei Yin, Michaël Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William T Freeman,
and Taesung Park. One-step diffusion with distribution matching distillation. In CVPR, 2024b. 3

Zhihang Yuan, Yuzhang Shang, Yue Song, Qiang Wu, Yan Yan, and Guangyu Sun. Asvd: Activation-
aware singular value decomposition for compressing large language models. arXiv preprint arXiv:
2312.05821, 2023. 4

Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image
diffusion models. In ICCV, 2023. 2

Qinsheng Zhang and Yongxin Chen. Fast sampling of diffusion models with exponential integrator.
In ICLR, 2022. 3

Qinsheng Zhang, Molei Tao, and Yongxin Chen. gddim: Generalized denoising diffusion implicit
models. In ICLR, 2022. 3

Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable
effectiveness of deep features as a perceptual metric. In CVPR, 2018. 8

Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, and Yuandong
Tian. GaLore: Memory-efficient LLM training by gradient low-rank projection. In Proceedings of
the 41st International Conference on Machine Learning, volume 235, pp. 61121–61143. PMLR,
2024a. 4

Tianchen Zhao, Tongcheng Fang, Enshu Liu, Wan Rui, Widyadewi Soedarmadji, Shiyao Li, Zinan
Lin, Guohao Dai, Shengen Yan, Huazhong Yang, et al. Vidit-q: Efficient and accurate quantization
of diffusion transformers for image and video generation. arXiv preprint arXiv:2406.02540, 2024b.
3, 7

Tianchen Zhao, Xuefei Ning, Tongcheng Fang, Enshu Liu, Guyue Huang, Zinan Lin, Shengen Yan,
Guohao Dai, and Yu Wang. Mixdq: Memory-efficient few-step text-to-image diffusion models
with metric-decoupled mixed precision quantization. arXiv preprint arXiv:2405.17873, 2024c. 3,
7

Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size Zheng, Luis Ceze, Arvind
Krishnamurthy, Tianqi Chen, and Baris Kasikci. Atom: Low-bit quantization for efficient and
accurate llm serving. MLSys, 2024d. 3

Xingyu Zheng, Haotong Qin, Xudong Ma, Mingyuan Zhang, Haojie Hao, Jiakai Wang, Zixiang Zhao,
Jinyang Guo, and Xianglong Liu. Binarydm: Towards accurate binarization of diffusion model.
arXiv preprint arXiv:2404.05662, 2024. 3

16



SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models

A MISSING PROOFS

A.1 PROOF OF PROPOSITION 4.1

Proof.

∥XW −Q(X)Q(W )∥F
= ∥XW −XQ(W ) +XQ(W )−Q(X)Q(W )∥F
≤∥X(W −Q(W ))∥F + ∥(X −Q(X))Q(W )∥F
≤∥X∥F ∥W −Q(W )∥F + ∥X −Q(X)∥F ∥Q(W )∥F
≤∥X∥F ∥W −Q(W )∥F + ∥X −Q(X)∥F ∥W − (W −Q(W ))∥F
≤∥X∥F ∥W −Q(W )∥F + ∥X −Q(X)∥F (∥W ∥F + ∥W −Q(W )∥F ).

A.2 PROOF OF PROPOSITION 4.2

Proof.

∥R−Q(R)∥F
= ∥R− sR ·QR∥F

=

∥∥∥∥sR · R

sR
− sR · round

(
R

sR

)∥∥∥∥
F

=|sR|
∥∥∥∥ R

sR
− round

(
R

sR

)∥∥∥∥
F

.

So,

E [∥R−Q(R)∥F ]

≤E [|sR|]
√

size(R)

=

√
size(R)

qmax
· E [max(|R|)]

≤
√

size(R)

qmax
· σ

√
2 log (size(R)), (8)

where σ is the std deviation of the normal distribution. Equation 8 comes from the maximal inequality
of Gaussian variables (Lemma 2.3 in Massart (2007)).

On the other hand,

E [∥R∥F ]

=E

√∑
x∈R

x2


≥E

[∑
x∈R |x|√
size(R)

]
(9)

=σ

√
2size(R)

π
, (10)

where Equation 9 comes from Cauchy-Schwartz inequality and Equation 10 comes from the expecta-
tion of half-normal distribution.

17



SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models

Together, we have

E [∥R−Q(R)∥F ]

≤
√

size(R)

qmax
· σ

√
2 log (size(R))

≤
√
log (size(R))π

qmax
E [∥R∥F ] .

B ADDITIONAL RESULTS

B.1 QUALITY RESULTS

We report extra quantitative quality results with additional metrics in Table 2. Specifically, CLIP
IQA (Wang et al., 2023) and CLIP Score (Hessel et al., 2021) assesses the image quality and text-
image alignment with CLIP (Radford et al., 2021), respectively. Structural Similarity Index Measure
(SSIM) is used to measure the luminance, contrast, and structure similarity of images produced by
our 4-bit model against the original 16-bit model. We also visualize more qualitative comparsions in
Figures 12, 13, 14, 15 and 16.

Table 2: Additional quantitative quality comparisons across different models. C.IQA means CLIP IQA, and
C.SCR means CLIP Score.

MJHQ sDCI

Backbone Model Precision Method Quality Similarity Quality Similarity

C.IQA (↑) C.SCR (↑) SSIM( ↑) C.IQA (↑) C.SCR (↑) SSIM (↑)

FLUX.1
-dev

(50 Steps)

BF16 – 0.952 26.0 – 0.955 25.4 –

INT W8A8 Ours 0.953 26.0 0.748 0.955 25.4 0.697

W4A16 NF4 0.947 25.8 0.748 0.951 25.4 0.697
INT W4A4 Ours 0.950 25.8 0.773 0.953 25.3 0.721
FP W4A4 Ours 0.950 25.8 0.780 0.952 25.3 0.727

FLUX.1
-schnell
(4 Steps)

BF16 – 0.938 26.6 – 0.932 26.2 –

INT W8A8 Ours 0.938 26.6 0.844 0.932 26.2 0.811

DiT W4A16 NF4 0.941 26.6 0.713 0.933 26.2 0.674
INT W4A4 Ours 0.939 26.5 0.693 0.932 26.2 0.647
FP W4A4 Ours 0.938 26.5 0.703 0.933 26.2 0.667

PixArt-Σ
(20 Steps)

FP16 – 0.944 26.8 – 0.966 26.1 –

INT W8A8 ViDiT-Q 0.948 26.7 0.815 0.966 26.1 0.756
INT W8A8 Ours 0.947 26.8 0.849 0.967 26.0 0.800

INT W4A8 ViDiT-Q 0.912 25.7 0.356 0.917 25.4 0.295
INT W4A4 ViDiT-Q 0.185 13.3 0.077 0.176 13.3 0.080
INT W4A4 Ours 0.927 26.6 0.602 0.952 26.1 0.519
FP W4A4 Ours 0.935 26.7 0.652 0.957 26.1 0.574

UNet

SDXL
-Turbo

(4 Steps)

FP16 – 0.926 26.5 – 0.913 26.5 –

INT W8A8 MixDQ 0.922 26.5 0.763 0.907 26.5 0.750
INT W8A8 Ours 0.925 26.5 0.821 0.912 26.5 0.808

INT W4A8 MixDQ 0.893 25.9 0.512 0.895 26.1 0.493
INT W4A4 MixDQ 0.556 13.1 0.289 0.548 11.9 0.296
INT W4A4 Ours 0.916 26.5 0.630 0.894 26.8 0.610
FP W4A4 Ours 0.919 26.4 0.640 0.901 26.7 0.620

SDXL
(30 Steps)

FP16 – 0.907 27.2 – 0.911 26.5 –

INT W8A8 TensorRT 0.905 26.7 0.733 0.901 26.1 0.697
INT W8A8 Ours 0.912 27.0 0.843 0.910 26.3 0.814

INT W4A4 Ours 0.916 26.5 0.630 0.894 26.8 0.610
FP W4A4 Ours 0.919 26.4 0.640 0.901 26.7 0.620

18



SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models

FLUX.1-dev BF16 
Image Reward: 0.953

Our INT W8A8 
Image Reward: 0.948

NF4 W4A16 
Image Reward: 0.910

Our INT W4A4 
Image Reward: 0.924

Our FP W4A4 
Image Reward: 0.932

Prompt: perfect, attractive, beautiful young italian mans face, Clear facial features, EyeLevel Shot, f1.8

Prompt: A scientist analyzing sequential data with a recurrent neural network A research laboratory with computer screens and graphs in the 
background Fluorescent lighting 35mm, photorealistic, Canon EOS 5D Mark IV DSLR, f5.6 aperture, 1125 second shutter speed, ISO 100

Prompt: Eiffel tower, landed on the moon, from moon perspective, earth in background, no town

Prompt: photography of word END in neon sign on a googie building by night

Figure 12: Qualitative visual results of FLUX.1-dev on MJHQ.

FLUX.1-schnell BF16 
Image Reward: 0.968

Our INT W8A8 
Image Reward: 0.966

NF4 W4A16 
Image Reward: 0.943

Our INT W4A4 
Image Reward: 0.965

Our FP W4A4 
Image Reward: 0.957

Prompt: an attuned eagle soaring over the wilderness sunset golden hour

Prompt: Ludwig van Beethoven playing modern electronic mulikeyboard Yamaha set, 8k, Shot on DIGITAL CINEMA VRAPTOR XL 8K VV 
Cinema Camera, f 11, Shutter Speed 1 800, 8mm lens, raw, super resolution, tone mapping, ray tracing, Megapixels

Prompt: ultra modern architure coffeeshop made with glass and with red flowers on the rims on the building

Prompt: cyberpunk lion with glowing eyes in the jungle hyperrealistic

Figure 13: Qualitative visual results of FLUX.1-schnell on MJHQ.

19



SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models

PixArt-  FP16 
Image Reward: 0.944

Σ ViDiT-Q INT W8A8 
Image Reward: 0.944

Our INT W8A8 
Image Reward: 0.955

ViDiT-Q INT W4A8 
Image Reward: 0.573

Our INT W4A4 
Image Reward: 0.898

Our FP W4A4 
Image Reward: 0.946

Prompt: Commercial photography of unlabelled omega 3 pills, with studio light, hyperdetailed, on 
black isolated plain, pro color grading, white lighting, Shot on 70mm lens, Canon camera, 8k v 5

Prompt: a 12 year old orphan boy wizard with tattered clothes. South American ancient 
clothing. Night sky with falling stars. Hyper realistic, cinematic lighting

Prompt: lake Powell at sunrise. Dramatic lighting with sun shining over the rocks. Still water. Realistic photograph. Breathtaking landscape. Ar 12

Prompt: 1950s style hamburger restaurant Cartoon with soft and funny contours with 3d with white background

Figure 14: Qualitative visual results of PixArt-Σ on MJHQ.

SDXL 
Image Reward: 0.729

TensorRT W8A8 
Image Reward: 0.591

Our W8A8 
Image Reward: 0.718

Our INT W4A4 
Image Reward: 0.591

Our FP W4A4 
Image Reward: 0.607

Prompt: tasty pancakes epic and realistic photo, isolated on dark background, photography f22 f1.4 

Prompt: morgan freeman headshot, hyperrealistic, 4k, colour graded, wearing old shashank redemption hat, looking at camera

Prompt: a portrait of a young lady in the rain, by guy aroch

Prompt: professional photo of a negroni cocktail. Italian atmosphere.

Figure 15: Qualitative visual results of SDXL on MJHQ.

20



SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models

SDXL-Turbo FP16 
Image Reward: 0.845

MixDQ INT W8A8 
Image Reward: 0.834

Our INT W8A8 
Image Reward: 0.845

MixDQ INT W4A8 
Image Reward: 0.708

Our INT W4A4 
Image Reward: 0.796

Our FP W4A4 
Image Reward: 0.822

Prompt: portrait of a miner after hard work in a coal mine, high contrast, a lot of details, good light, a mining shaft in the background, 
Canon EOS R5 prime lens, the lighting is a mix of natural light and artificial lighting, creating a dramatic and intense effect.

Prompt: barcelone conference event blockchain summit

Prompt: AI, flaming lion with a human body, warrior, fighting pose, 8k, 10 PIC, a photorealistic 
white tiger, emerging from the jungle, stalking its prey in the snow

Prompt: lemur with long legs and red lips, natural habitat. Hyperrealistic, photo quality

Figure 16: Qualitative visual results of SDXL-Turbo on MJHQ.

21



SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models

B.2 LORA RESULTS

In Figure 17, we showcase more visual results of applying the aforementioned five community-
contributed LoRAs of different styles (Realism, Ghibsky Illustration, Anime, Children Sketch, and
Yarn Art) to our INT4 quantized models.

(a) Realism LoRA

(e) Yarn Art LoRA

(c) Anime LoRA

(d) Children Sketch LoRA

(b) Ghibsky Illustration LoRA

FLUX.1-dev 
BF16

Our INT4

FLUX.1-dev 
BF16

Our INT4

FLUX.1-dev 
BF16

Our INT4

FLUX.1-dev 
BF16

Our INT4

FLUX.1-dev 
BF16

Our INT4

Figure 17: Additional LoRA results on FLUX.1-dev. When applying LoRAs, our INT4 model matches the
image quality of the original BF16 model. See Appendix C for the detailed used text prompts.

22

https://huggingface.co/XLabs-AI/flux-RealismLora
https://huggingface.co/aleksa-codes/flux-ghibsky-illustration
https://huggingface.co/alvdansen/sonny-anime-fixed
https://huggingface.co/Shakker-Labs/FLUX.1-dev-LoRA-Children-Simple-Sketch
https://huggingface.co/linoyts/yarn_art_Flux_LoRA


SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models

C TEXT PROMPTS

Below we provide the text prompts we use in Figure 9 (from left to right).

a man in armor with a beard and a sword
GHIBSKY style, a fisherman casting a line into a peaceful village lake

surrounded by quaint cottages↪→
girl, neck tuft, white hair, sheep horns, blue eyes, nm22 style
sketched style, A squirrel wearing glasses and reading a tiny book under

an oak tree↪→
a panda playing in the snow, yarn art style

The text prompts we use in Figure 17 are (in the rasterizing order):

A male secret agent in a tuxedo, holding a gun, standing in front of a
burning building↪→

A handsome man in a suit, 25 years old, cool, futuristic
A knight in shining armor, standing in front of a castle under siege
A knight fighting a fire-breathing dragon in front of a medieval castle,

flames and smoke↪→
A male wizard with a long white beard casting a lightning spell in the

middle of a storm↪→
A young woman with long flowing hair, standing on a mountain peak at dawn,

overlooking a misty valley↪→

GHIBSKY style, a cat on a windowsill gazing out at a starry night sky and
distant city lights↪→

GHIBSKY style, a quiet garden at twilight, with blooming flowers and the
soft glow of lanterns lighting up the path↪→

GHIBSKY style, a serene mountain lake with crystal-clear water,
surrounded by towering pine trees and rocky cliffs↪→

GHIBSKY style, an enchanted forest at night, with glowing mushrooms and
fireflies lighting up the underbrush↪→

GHIBSKY style, a peaceful beach town with colorful houses lining the
shore and a calm ocean stretching out into the horizon↪→

GHIBSKY style, a cozy living room with a view of a snow-covered forest,
the fireplace crackling and a blanket draped over a comfy chair↪→

a dog wearing a wizard hat, nm22 anime style
a girl looking at the stars, nm22 anime style
a fish swimming in a pond, nm22 style
a giraffe with a long scarf, nm22 style
a bird sitting on a branch, nm22 minimalist style
a girl wearing a flower crown, nm22 style

sketched style, A garden full of colorful butterflies and blooming
flowers with a gentle breeze blowing↪→

sketched style, A beach scene with kids building sandcastles and seagulls
flying overhead↪→

sketched style, A hot air balloon drifting peacefully over a patchwork of
fields and forests below↪→

sketched style, A sunny meadow with a girl in a flowy dress chasing
butterflies↪→

sketched style, A little boy dressed as a pirate, steering a toy ship on
a small stream↪→

sketched style, A small boat floating on a peaceful lake, surrounded by
trees and mountains↪→

a hot air balloon flying over mountains, yarn art style
a cat chasing a butterfly, yarn art style
a squirrel collecting acorns, yarn art style
a wizard casting a spell, yarn art style
a jellyfish floating in the ocean, yarn art style
a sea turtle swimming through a coral reef, yarn art style

23


	Introduction
	Related Work
	Quantization Preliminary
	Method
	Problem Formulation
	SVDQuant: Absorbing Outliers via Low-Rank Branch
	Nunchaku: Fusing Low-Rank and Low-Bit Branch Kernels

	Experiments
	Setups
	Results

	Conclusion & Discussion
	Missing Proofs
	Proof of Proposition 4.1
	Proof of Proposition 4.2

	Additional Results
	Quality Results
	LoRA Results

	Text Prompts

